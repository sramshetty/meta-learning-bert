{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to build an accurate sentiment analysis model with handful training examples\n",
    "\n",
    "- Suppose we want to train a model to classify product reviews into 2 categories: positive and negative.\n",
    "- We use a single model for each type of product (or domain).\n",
    "- However, in some domains, we only have a limited number of training examples (low-resource domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"GOOD LOOKING KICKS IF YOUR KICKIN IT OLD SCHOOL LIKE ME. AND COMFORTABLE. AND RELATIVELY CHEAP. I'LL ALWAYS KEEP A PAIR OF STAN SMITH'S AROUND FOR WEEKENDS\",\n",
       "  'label': 'positive',\n",
       "  'domain': 'apparel'},\n",
       " {'text': 'These sunglasses are all right. They were a little crooked, but still cool..',\n",
       "  'label': 'positive',\n",
       "  'domain': 'apparel'},\n",
       " {'text': \"I don't see the difference between these bodysuits and the more expensive ones. Fits my boy just right\",\n",
       "  'label': 'positive',\n",
       "  'domain': 'apparel'},\n",
       " {'text': 'Very nice basic clothing. I think the size is fine. I really like being able to find these shades of green, though I have decided the lighter shade is really a feminine color. This is the only brand that I can find these muted greens',\n",
       "  'label': 'positive',\n",
       "  'domain': 'apparel'},\n",
       " {'text': 'I love these socks. They fit great (my 15 month old daughter has thick ankles) and she can zoom around on the kitchen floor and not take a nose dive into things. :',\n",
       "  'label': 'positive',\n",
       "  'domain': 'apparel'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let inspect the data\n",
    "import json\n",
    "from random import shuffle\n",
    "reviews = json.load(open('dataset.json'))\n",
    "\n",
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'apparel': 1717,\n",
       "         'baby': 1107,\n",
       "         'beauty': 993,\n",
       "         'books': 921,\n",
       "         'camera_&_photo': 1086,\n",
       "         'cell_phones_&_service': 698,\n",
       "         'dvd': 893,\n",
       "         'electronics': 1277,\n",
       "         'grocery': 1100,\n",
       "         'health_&_personal_care': 1429,\n",
       "         'jewelry_&_watches': 1086,\n",
       "         'kitchen_&_housewares': 1390,\n",
       "         'magazines': 1133,\n",
       "         'music': 1007,\n",
       "         'outdoor_living': 980,\n",
       "         'software': 1029,\n",
       "         'sports_&_outdoors': 1336,\n",
       "         'toys_&_games': 1363,\n",
       "         'video': 1010,\n",
       "         'automotive': 100,\n",
       "         'computer_&_video_games': 100,\n",
       "         'office_products': 100})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "mention_domain = [r['domain'] for r in reviews]\n",
    "Counter(mention_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the statistics, we have 100 training examples for \"office_products\", \"automotive\", \"computer_&_video_games\"\n",
    "Can we still build an accurate model on these domain ? --> Absolutetly\n",
    "#### Solution: We leverage data from high-resource domains to create a good \"starting point\". And from this point, we start training a specific model for low-resource domain \n",
    " - Approach #1: Transfer learning.: We train a single model (Model_X) on concatenate data from high-resource domains. Then, we retrain Model_X on low-resource domain\n",
    "\n",
    "\n",
    " - Approach #2: Meta learning: We stimulate a lot of situations where the Model_X are forced to learn fast with only few training examples. The model_X are getting better at \"learning with less\" after each training situation. We called these situations as Meta-task. Each task contain two sets:\n",
    "   - Support set: contain few training samples\n",
    "   - Query set: Provide learning feedback. The model use this feedback to adapt its learning strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let create meta learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import json, pickle\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "LABEL_MAP  = {'positive':0, 'negative':1, 0:'positive', 1:'negative'}\n",
    "# LABEL_MAP = {'apparel': 0,\n",
    "#          'baby': 1,\n",
    "#          'beauty': 2,\n",
    "#          'books': 3,\n",
    "#          'camera_&_photo': 4,\n",
    "#          'cell_phones_&_service': 5,\n",
    "#          'dvd': 6,\n",
    "#          'electronics': 7,\n",
    "#          'grocery': 8,\n",
    "#          'health_&_personal_care': 9,\n",
    "#          'jewelry_&_watches': 10,\n",
    "#          'kitchen_&_housewares': 11,\n",
    "#          'magazines': 12,\n",
    "#          'music': 13,\n",
    "#          'outdoor_living': 14,\n",
    "#          'software': 15,\n",
    "#          'sports_&_outdoors': 16,\n",
    "#          'toys_&_games': 17,\n",
    "#          'video': 18,\n",
    "#          'automotive': 19,\n",
    "#          'computer_&_video_games': 20,\n",
    "#          'office_products': 21}\n",
    "\n",
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 256\n",
    "        self.create_batch(self.num_task)\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            domain = random.choice(self.examples)['domain']\n",
    "            domainExamples = [e for e in self.examples if e['domain'] == domain]\n",
    "            \n",
    "            # 1.select k_support + k_query examples from domain randomly\n",
    "            selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
    "            random.shuffle(selected_examples)\n",
    "            exam_train = selected_examples[:self.k_support]\n",
    "            exam_test  = selected_examples[self.k_support:]\n",
    "            \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = tokenizer.encode(example['text'])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            label_id = LABEL_MAP[example['label']] # LABEL_MAP[example['domain']]\n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split meta training and meta testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21555 300\n"
     ]
    }
   ],
   "source": [
    "low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
    "train_examples = [r for r in reviews if r['domain'] not in low_resource_domains]\n",
    "test_examples = [r for r in reviews if r['domain'] in low_resource_domains]\n",
    "print(len(train_examples), len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "train = MetaTask(train_examples, num_task = 50, k_support=20, k_query=20, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'My dog loves this toy. She especially loves for us to hide treats inside it. She will find a way to remove the treats without taking any of the squirrels out',\n",
       "  'label': 'positive',\n",
       "  'domain': 'kitchen_&_housewares'},\n",
       " {'text': \"My daughter used this set and now my son uses it. We threw out the fork by mistake so I'm ordering another set -- the fork is just perfect for a toddler but my almost 6-year old still loves the fork. I'd highly recommend this\",\n",
       "  'label': 'positive',\n",
       "  'domain': 'kitchen_&_housewares'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a glance at the first two samples from support set of 1st meta-task\n",
    "train.supports[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.TensorDataset at 0x7f33777deaf0>,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x7f33777de3d0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of the 1st meta-task. It contains two TensorDataset: support set and query set\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2026,  3899,  7459,  2023,  9121,  1012,  2016,  2926,  7459,\n",
       "           2005,  2149,  2000,  5342, 18452,  2503,  2009,  1012,  2016,  2097,\n",
       "           2424,  1037,  2126,  2000,  6366,  1996, 18452,  2302,  2635,  2151,\n",
       "           1997,  1996, 29384,  2041,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2026,  2684,  2109,  2023,  2275,  1998,  2085,  2026,  2365,\n",
       "           3594,  2009,  1012,  2057,  4711,  2041,  1996,  9292,  2011,  6707,\n",
       "           2061,  1045,  1005,  1049, 13063,  2178,  2275,  1011,  1011,  1996,\n",
       "           9292,  2003,  2074,  3819,  2005,  1037,  6927,  3917,  2021,  2026,\n",
       "           2471,  1020,  1011,  2095,  2214,  2145,  7459,  1996,  9292,  1012,\n",
       "           1045,  1005,  1040,  3811, 16755,  2023,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let take a look at the first two samples from support set\n",
    "train[0][0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0,len(idxs), batch_size):\n",
    "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
    "\n",
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        self.num_labels = 2\n",
    "        self.meta_epoch=10\n",
    "        self.k_spt=20\n",
    "        self.k_qry=20\n",
    "        self.outer_batch_size = 2\n",
    "        self.inner_batch_size = 12\n",
    "        self.outer_update_lr = 5e-5\n",
    "        self.inner_update_lr = 5e-5\n",
    "        self.inner_update_step = 10\n",
    "        self.inner_update_step_eval = 40\n",
    "        self.bert_model = 'bert-base-uncased'\n",
    "        self.num_task_train = 20\n",
    "        self.num_task_test = 5\n",
    "\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertForSequenceClassification\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.outer_batch_size = args.outer_batch_size\n",
    "        self.inner_batch_size = args.inner_batch_size\n",
    "        self.outer_update_lr  = args.outer_update_lr\n",
    "        self.inner_update_lr  = args.inner_update_lr\n",
    "        self.inner_update_step = args.inner_update_step\n",
    "        self.inner_update_step_eval = args.inner_update_step_eval\n",
    "        self.bert_model = args.bert_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
    "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, batch_tasks, training = True):\n",
    "        \"\"\"\n",
    "        batch = [(support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset)]\n",
    "        \n",
    "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
    "        \"\"\"\n",
    "        task_accs = []\n",
    "        sum_gradients = []\n",
    "        num_task = len(batch_tasks)\n",
    "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
    "\n",
    "        for task_id, task in enumerate(batch_tasks):\n",
    "            support = task[0]\n",
    "            query   = task[1]\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.device)\n",
    "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
    "                                            batch_size=self.inner_batch_size)\n",
    "            \n",
    "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
    "            fast_model.train()\n",
    "            \n",
    "            print('----Task',task_id, '----')\n",
    "            for i in range(0,num_inner_update_step):\n",
    "                all_loss = []\n",
    "                for inner_step, batch in enumerate(support_dataloader):\n",
    "                    \n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
    "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
    "                    \n",
    "                    loss = outputs[0]              \n",
    "                    loss.backward()\n",
    "                    inner_optimizer.step()\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    \n",
    "                    all_loss.append(loss.item())\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
    "            \n",
    "            fast_model.to(torch.device('cpu'))\n",
    "            \n",
    "            if training:\n",
    "                meta_weights = list(self.model.parameters())\n",
    "                fast_weights = list(fast_model.parameters())\n",
    "\n",
    "                gradients = []\n",
    "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
    "                    gradient = meta_params - fast_params\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(gradient)\n",
    "                    else:\n",
    "                        sum_gradients[i] += gradient\n",
    "\n",
    "            fast_model.to(self.device)\n",
    "            fast_model.eval()\n",
    "            with torch.no_grad():\n",
    "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
    "                query_batch = iter(query_dataloader).next()\n",
    "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
    "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
    "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
    "\n",
    "                q_logits = F.softmax(q_outputs[1],dim=1)\n",
    "                pre_label_id = torch.argmax(q_logits,dim=1)\n",
    "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
    "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
    "\n",
    "                acc = accuracy_score(pre_label_id,q_label_id)\n",
    "                task_accs.append(acc)\n",
    "            \n",
    "            fast_model.to(torch.device('cpu'))\n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if training:\n",
    "            # Average gradient across tasks\n",
    "            for i in range(0,len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
    "\n",
    "            #Assign gradient for original model, then using optimizer to update its weights\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "        \n",
    "        return np.mean(task_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)\n",
    "test = MetaTask(test_examples, num_task = 5, k_support=20, k_query=20, tokenizer = tokenizer)\n",
    "random_seed(int(time.time() % 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'You can read that the Eyetoy is a great \"toy\" in other reviews, but I just wanted to say that you can use it as a webcam on your PC by just plugging it into your computer\\'s USB port',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'The best way to play this game is to skip the missions and just start havoc in the city. Fight the gangs and police, attack the citizens and steal cars!! If you are able to find some of the cheat codes off the Internet, then this is where the fun really begins! These secrets can give you armor, weapons and even a tank (this is only listing a few)! As always parents, research a video game before you buy it for your 7 year old.',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"I got this game during the summer expecting it to be great fun. Well...it caught my interest for a few weeks then I just got bored with it. I'd only recomend it to people who do not get bored easily\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'it is a pretty fun game with a few quirks here and there. Some of them include the anoying \"click button\" interface, and a few problems that are just a random guess to solve. Other than that, it has very good graphics and a very addictive storyline',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'At first playing a Star Wars game made me feel very nerdy, but let me tell you if this is what nerdy is then it rocks!!!!! This game is incredible!!! It is very, very fun basically you choose what side you want and destroy the other what with gaining the several checkpoints that are throughout it. Buy this game and put your blasters on full',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'This game is horrible. I rented it for 5 days and returned it 3 hours after i rented it. I had wasted 5 bucks. Its patheticic. You cant do anything, graphics are horrble, cutscenes are as long as some commercials, and the characters are gay. Pros: Sunny can bite, yayCons: Horrbiel graphicsonly 3 playable charactersBad humorShort cutscenesu cant do anything1 path gamehardly 3d, (seriously, it doesnt look like it',\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"We got this game free in a box of Corn Chex. We thought, Oh this will be fun since the show is cool. So we got home and played it and it was so annoying! There's just like a black screen before and after the question shows up and a black background. Just, don't buy it if you want to have any fun\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"If you are looking for codes like Game Shark had, this is not for you. Not only do you HAVE to have a computer, If anything happens to the disk, you are screwed. You won't be able reinstall windows, or install it on a new computer. Some games do offer rewards like unlimited health for %100 completion. So if those are they games you plan on using, this is the best thing out there\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'This game is a great one. I enjoyed this game it is one of my favorites. I recommend this game for anyone who enjoys Final Fantasy or a good RPG',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'This extension cable did not work with my standard Microsoft S Controller. Unless you still use the old monster-sized XBox controller, these cables are worthless',\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"I've played them all and enjoyed them all. Sorry this was the last one\",\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'Used this product to softmod my xbox. Had to download an older version of the software though. If you just use it to cheat in games..probably not worth the money.',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"I've only had this controller for a few days, and I have found that the layout is the same as the black controller. I do wish that the thumb controls were a little softer, as they hurt my thunb while playing an exciting game\",\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"If you need a remote to turn on the tele and run the 360, this will do. But it won't operate a DVR, DVD, VCR or cable box. If you're really keen on it, I'll sell you mine.\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"I saw a DVD player at Target for $30 yesterday. I especially don't see the need for the xbox 360 remote if it's $40!! Space-saving, maybe???\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'The memory cards came exactly as advertised and they worked great. No read errors, VERY noticeable increased speed of load time compared to other card types..would recommend to anyone looking for a value pack of memory cards because this was well worth it',\n",
       "  'label': 'positive',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"I bought the KR Party bundle for my boyfriend's daughter for her birthday. She loves the game, but it has only been about two weeks and the mic is dead. It started with the static for a while, but then while I was singing, it just died completely. I know I'm not a great singer, but I'm not THAT bad\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'As was said in a earlier review the creators of final fantasy are giving us everything but what we actually asked for. They gave us Advent Children, cell phone games, and finally Dirge of Cerberus. This game is an action based game showing you a day in the life of the most under-used and least interesting character in the Final Fantasy Series. Its the creators way of telling us that they will never re-make Final Fantasy VII but will rather insult us with every marketing ploy know to man',\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': \"nothing more really needs to be said.I'm running a 3GHz P4;256Mb ATI Radeon 9000 series PCI express video card and 2Gb of RAM and the game still hangs,lags and locks up on my computer. I tried uninstalling & reinstalling-same effect.DO NOT BUY THIS GAM\",\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'},\n",
       " {'text': 'I am extreamly dissapointd with this game. Every section and mission is exactly the same with the exception of the location. There is absolutly no skill involved in beating the game which i was able to do in 3 sittings.',\n",
       "  'label': 'negative',\n",
       "  'domain': 'computer_&_video_games'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.supports[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  0.794634997844696\n",
      "Inner Loss:  0.47190433740615845\n",
      "Inner Loss:  0.22601135820150375\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7326231896877289\n",
      "Inner Loss:  0.46853138506412506\n",
      "Inner Loss:  0.2765398249030113\n",
      "Step: 0 \ttraining Acc: 0.65\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7966306209564209\n",
      "Inner Loss:  0.5776962637901306\n",
      "Inner Loss:  0.20834548771381378\n",
      "Inner Loss:  0.04520825482904911\n",
      "Inner Loss:  0.009610104374587536\n",
      "Inner Loss:  0.003898588242009282\n",
      "Inner Loss:  0.002302575740031898\n",
      "Inner Loss:  0.0016317322151735425\n",
      "Inner Loss:  0.0013634886126965284\n",
      "Inner Loss:  0.0011220095329917967\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7329597473144531\n",
      "Inner Loss:  0.22448144108057022\n",
      "Inner Loss:  0.06185204163193703\n",
      "Inner Loss:  0.017484406009316444\n",
      "Inner Loss:  0.00848896661773324\n",
      "Inner Loss:  0.00460864813067019\n",
      "Inner Loss:  0.0032084144186228514\n",
      "Inner Loss:  0.0026050308952108026\n",
      "Inner Loss:  0.0021070591174066067\n",
      "Inner Loss:  0.0017700588214211166\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8208121359348297\n",
      "Inner Loss:  0.5636282563209534\n",
      "Inner Loss:  0.2660120576620102\n",
      "Inner Loss:  0.0903225727379322\n",
      "Inner Loss:  0.028708972968161106\n",
      "Inner Loss:  0.012571715284138918\n",
      "Inner Loss:  0.007160991197451949\n",
      "Inner Loss:  0.004499270115047693\n",
      "Inner Loss:  0.0034311057534068823\n",
      "Inner Loss:  0.0025680739199742675\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7385943233966827\n",
      "Inner Loss:  0.43577101826667786\n",
      "Inner Loss:  0.1067684181034565\n",
      "Inner Loss:  0.03055981919169426\n",
      "Inner Loss:  0.00974842207506299\n",
      "Inner Loss:  0.0043005787301808596\n",
      "Inner Loss:  0.0029550519539043307\n",
      "Inner Loss:  0.002358275931328535\n",
      "Inner Loss:  0.0016845460049808025\n",
      "Inner Loss:  0.001439847459550947\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7691195905208588\n",
      "Inner Loss:  0.272660493850708\n",
      "Inner Loss:  0.0644563790410757\n",
      "Inner Loss:  0.018662553280591965\n",
      "Inner Loss:  0.006925977300852537\n",
      "Inner Loss:  0.003653916297480464\n",
      "Inner Loss:  0.0024046958424150944\n",
      "Inner Loss:  0.0019395839772187173\n",
      "Inner Loss:  0.0014273708802647889\n",
      "Inner Loss:  0.0012920137960463762\n",
      "Step: 0 Test F1: 0.76\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.567349910736084\n",
      "Inner Loss:  0.32485708594322205\n",
      "Inner Loss:  0.1500537395477295\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6812946498394012\n",
      "Inner Loss:  0.1815868690609932\n",
      "Inner Loss:  0.04508405365049839\n",
      "Step: 1 \ttraining Acc: 0.7\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7283960580825806\n",
      "Inner Loss:  0.326621875166893\n",
      "Inner Loss:  0.05717821791768074\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7040578126907349\n",
      "Inner Loss:  0.3030948042869568\n",
      "Inner Loss:  0.08304021880030632\n",
      "Step: 2 \ttraining Acc: 0.675\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7091420590877533\n",
      "Inner Loss:  0.310624822974205\n",
      "Inner Loss:  0.07982197776436806\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6135281920433044\n",
      "Inner Loss:  0.13758765906095505\n",
      "Inner Loss:  0.040808334946632385\n",
      "Step: 3 \ttraining Acc: 0.725\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8178254067897797\n",
      "Inner Loss:  0.20476170629262924\n",
      "Inner Loss:  0.06357410736382008\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7047638595104218\n",
      "Inner Loss:  0.2560104429721832\n",
      "Inner Loss:  0.06995757296681404\n",
      "Step: 4 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6068151593208313\n",
      "Inner Loss:  0.07118641957640648\n",
      "Inner Loss:  0.01956088002771139\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6222607493400574\n",
      "Inner Loss:  0.10979023203253746\n",
      "Inner Loss:  0.02697252854704857\n",
      "Step: 5 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.46205346286296844\n",
      "Inner Loss:  0.0922207199037075\n",
      "Inner Loss:  0.023926389403641224\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6734297275543213\n",
      "Inner Loss:  0.13273956254124641\n",
      "Inner Loss:  0.03846082277595997\n",
      "Step: 6 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.39580102264881134\n",
      "Inner Loss:  0.0466040913015604\n",
      "Inner Loss:  0.01736428774893284\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8224228322505951\n",
      "Inner Loss:  0.12836305797100067\n",
      "Inner Loss:  0.0411249790340662\n",
      "Step: 7 \ttraining Acc: 0.8\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4363558143377304\n",
      "Inner Loss:  0.03732473775744438\n",
      "Inner Loss:  0.01166109461337328\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6416792869567871\n",
      "Inner Loss:  0.25635069608688354\n",
      "Inner Loss:  0.10521815903484821\n",
      "Step: 8 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6139471977949142\n",
      "Inner Loss:  0.073651272803545\n",
      "Inner Loss:  0.02665125485509634\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6108002066612244\n",
      "Inner Loss:  0.050300780683755875\n",
      "Inner Loss:  0.019500788301229477\n",
      "Step: 9 \ttraining Acc: 0.775\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.3150816932320595\n",
      "Inner Loss:  0.38678543269634247\n",
      "Inner Loss:  0.09365500137209892\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.28420185297727585\n",
      "Inner Loss:  0.027982167899608612\n",
      "Inner Loss:  0.00811263034120202\n",
      "Step: 10 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.9367454424500465\n",
      "Inner Loss:  0.0646342784166336\n",
      "Inner Loss:  0.038352517411112785\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.798675037920475\n",
      "Inner Loss:  0.10597900673747063\n",
      "Inner Loss:  0.04972339980304241\n",
      "Step: 11 \ttraining Acc: 0.8999999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6681834757328033\n",
      "Inner Loss:  0.05643545463681221\n",
      "Inner Loss:  0.033744266256690025\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6183109730482101\n",
      "Inner Loss:  0.05277632735669613\n",
      "Inner Loss:  0.021280732937157154\n",
      "Step: 12 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2769118472933769\n",
      "Inner Loss:  0.05110643431544304\n",
      "Inner Loss:  0.017360194120556116\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.4231809079647064\n",
      "Inner Loss:  0.023289828561246395\n",
      "Inner Loss:  0.008801172953099012\n",
      "Step: 13 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7351546287536621\n",
      "Inner Loss:  0.09229327365756035\n",
      "Inner Loss:  0.03253285959362984\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.1101105771958828\n",
      "Inner Loss:  0.014702128246426582\n",
      "Inner Loss:  0.005895880167372525\n",
      "Step: 14 \ttraining Acc: 0.8\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5020036399364471\n",
      "Inner Loss:  0.033496616408228874\n",
      "Inner Loss:  0.012955221347510815\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.40510424226522446\n",
      "Inner Loss:  0.03619951568543911\n",
      "Inner Loss:  0.012245836667716503\n",
      "Step: 15 \ttraining Acc: 0.95\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.35726049169898033\n",
      "Inner Loss:  0.033599453046917915\n",
      "Inner Loss:  0.012794860638678074\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7883126586675644\n",
      "Inner Loss:  0.04407655820250511\n",
      "Inner Loss:  0.020343300886452198\n",
      "Step: 16 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.30735698342323303\n",
      "Inner Loss:  0.09941466525197029\n",
      "Inner Loss:  0.011744364630430937\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.600885346531868\n",
      "Inner Loss:  0.058739421889185905\n",
      "Inner Loss:  0.01486945478245616\n",
      "Step: 17 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7994700372219086\n",
      "Inner Loss:  0.15835237130522728\n",
      "Inner Loss:  0.037111202254891396\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.30915896594524384\n",
      "Inner Loss:  0.04778682254254818\n",
      "Inner Loss:  0.013002480380237103\n",
      "Step: 18 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7277404367923737\n",
      "Inner Loss:  0.06300767324864864\n",
      "Inner Loss:  0.02147696539759636\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8984897136688232\n",
      "Inner Loss:  0.06564728915691376\n",
      "Inner Loss:  0.019168052822351456\n",
      "Step: 19 \ttraining Acc: 0.975\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.45817504823207855\n",
      "Inner Loss:  0.03700967971235514\n",
      "Inner Loss:  0.02024741843342781\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9906973540782928\n",
      "Inner Loss:  0.06650393083691597\n",
      "Inner Loss:  0.022986465133726597\n",
      "Step: 20 \ttraining Acc: 0.825\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6312320381402969\n",
      "Inner Loss:  0.02568791899830103\n",
      "Inner Loss:  0.013386155478656292\n",
      "Inner Loss:  0.006326774368062615\n",
      "Inner Loss:  0.003651339327916503\n",
      "Inner Loss:  0.0022219630191102624\n",
      "Inner Loss:  0.001619894930627197\n",
      "Inner Loss:  0.001308033533859998\n",
      "Inner Loss:  0.0009384508011862636\n",
      "Inner Loss:  0.0007309987850021571\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7938297986984253\n",
      "Inner Loss:  0.07150550931692123\n",
      "Inner Loss:  0.023923690430819988\n",
      "Inner Loss:  0.013718151487410069\n",
      "Inner Loss:  0.007116246502846479\n",
      "Inner Loss:  0.004145675105974078\n",
      "Inner Loss:  0.0031151213916018605\n",
      "Inner Loss:  0.0024917066330090165\n",
      "Inner Loss:  0.0019737554830498993\n",
      "Inner Loss:  0.0016067922697402537\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6880104243755341\n",
      "Inner Loss:  0.05282559245824814\n",
      "Inner Loss:  0.020034851506352425\n",
      "Inner Loss:  0.012063637841492891\n",
      "Inner Loss:  0.005979717010632157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Loss:  0.0036600210005417466\n",
      "Inner Loss:  0.0025735810631886125\n",
      "Inner Loss:  0.0018433818477205932\n",
      "Inner Loss:  0.001521672762464732\n",
      "Inner Loss:  0.0013695580419152975\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8910636454820633\n",
      "Inner Loss:  0.1942700482904911\n",
      "Inner Loss:  0.029756061732769012\n",
      "Inner Loss:  0.015381328761577606\n",
      "Inner Loss:  0.008304782211780548\n",
      "Inner Loss:  0.004991145338863134\n",
      "Inner Loss:  0.0037633045576512814\n",
      "Inner Loss:  0.0032402535434812307\n",
      "Inner Loss:  0.0024294236209243536\n",
      "Inner Loss:  0.0018485342152416706\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6066574454307556\n",
      "Inner Loss:  0.035717785358428955\n",
      "Inner Loss:  0.02155944984406233\n",
      "Inner Loss:  0.009249984752386808\n",
      "Inner Loss:  0.006347376387566328\n",
      "Inner Loss:  0.0036222548224031925\n",
      "Inner Loss:  0.0024354412453249097\n",
      "Inner Loss:  0.0020789207192137837\n",
      "Inner Loss:  0.0016043579089455307\n",
      "Inner Loss:  0.0014227096689864993\n",
      "Step: 20 Test F1: 0.8299999999999998\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5872394293546677\n",
      "Inner Loss:  0.03250002674758434\n",
      "Inner Loss:  0.013444751035422087\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.027358300052583218\n",
      "Inner Loss:  0.12485404207836837\n",
      "Inner Loss:  0.0023072328185662627\n",
      "Step: 21 \ttraining Acc: 0.85\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.032769910991191864\n",
      "Inner Loss:  0.010056078201159835\n",
      "Inner Loss:  0.0031925401417538524\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.18871666863560677\n",
      "Inner Loss:  0.007754249731078744\n",
      "Inner Loss:  0.0024637168971821666\n",
      "Step: 22 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6085302531719208\n",
      "Inner Loss:  0.050308583304286\n",
      "Inner Loss:  0.015799615066498518\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.2812869269400835\n",
      "Inner Loss:  0.014529441483318806\n",
      "Inner Loss:  0.006908218376338482\n",
      "Step: 23 \ttraining Acc: 0.8\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.987290620803833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-45646a877aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ttraining Acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f2675dbeff1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_tasks, training)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.meta_epoch):\n",
    "    \n",
    "    train = MetaTask(train_examples, num_task = 50, k_support=20, k_query=20, tokenizer = tokenizer)\n",
    "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "    for step, task_batch in enumerate(db):\n",
    "        \n",
    "        f = open('log.txt', 'a')\n",
    "        \n",
    "        acc = learner(task_batch)\n",
    "        \n",
    "        print('Step:', step, '\\ttraining Acc:', acc)\n",
    "        f.write(str(acc) + '\\n')\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            random_seed(123)\n",
    "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "            acc_all_test = []\n",
    "\n",
    "            for test_batch in db_test:\n",
    "                acc = learner(test_batch, training = False)\n",
    "                acc_all_test.append(acc)\n",
    "\n",
    "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
    "            \n",
    "            random_seed(int(time.time() % 10))\n",
    "        \n",
    "        global_step += 1\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'good looking kicks if your kickin it old school like me and comfortable ill relatively cheap and always keep a pair of smiths stan around for weekends'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'good looking kicks if your kickin it old school like me and comfortable and relatively cheap ill always keep a pair of stan smiths around for weekend'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'good looking kicks if your kickin it old school like me and comfortable and relatively cheap ill always keep a partner off of stan smiths around for weekends'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'good looking kicks if your kickin of old school like me and comfortable and relatively cheap weekends always keep a pair it stan smiths around for ill'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'good looking kicks if your kickin it old school like me and comfortable and cheap ill always keep a pair of stan around for weekends'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = json.load(open('aug_dataset.json'))\n",
    "\n",
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176871 2466\n"
     ]
    }
   ],
   "source": [
    "low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
    "train_examples = [r for r in reviews if r['domain'] not in low_resource_domains]\n",
    "test_examples = [r for r in reviews if r['domain'] in low_resource_domains]\n",
    "print(len(train_examples), len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'apparel': 14067,\n",
       "         'baby': 9082,\n",
       "         'beauty': 8130,\n",
       "         'books': 7568,\n",
       "         'camera_&_photo': 8909,\n",
       "         'cell_phones_&_service': 5736,\n",
       "         'dvd': 7328,\n",
       "         'electronics': 10473,\n",
       "         'grocery': 9047,\n",
       "         'health_&_personal_care': 11703,\n",
       "         'jewelry_&_watches': 8908,\n",
       "         'kitchen_&_housewares': 11393,\n",
       "         'magazines': 9301,\n",
       "         'music': 8268,\n",
       "         'outdoor_living': 8045,\n",
       "         'software': 8448,\n",
       "         'sports_&_outdoors': 10962,\n",
       "         'toys_&_games': 11221,\n",
       "         'video': 8282,\n",
       "         'automotive': 820,\n",
       "         'computer_&_video_games': 820,\n",
       "         'office_products': 826})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_domain = [r['domain'] for r in reviews]\n",
    "Counter(mention_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "train = MetaTask(train_examples, num_task = 50, k_support=50, k_query=20, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "aug_learner = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)\n",
    "test = MetaTask(test_examples, num_task = 5, k_support=20, k_query=20, tokenizer = tokenizer)\n",
    "random_seed(int(time.time() % 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  0.7163063585758209\n",
      "Inner Loss:  0.26684311032295227\n",
      "Inner Loss:  0.07934820279479027\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7242736518383026\n",
      "Inner Loss:  0.3643168658018112\n",
      "Inner Loss:  0.14608646929264069\n",
      "Step: 0 \ttraining Acc: 0.8999999999999999\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5768259167671204\n",
      "Inner Loss:  0.17719973623752594\n",
      "Inner Loss:  0.0569253321737051\n",
      "Inner Loss:  0.023155110888183117\n",
      "Inner Loss:  0.012182001490145922\n",
      "Inner Loss:  0.007312146481126547\n",
      "Inner Loss:  0.004688662709668279\n",
      "Inner Loss:  0.0039952805964276195\n",
      "Inner Loss:  0.003227527136914432\n",
      "Inner Loss:  0.0029161011334508657\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6728940308094025\n",
      "Inner Loss:  0.15890464186668396\n",
      "Inner Loss:  0.0582321397960186\n",
      "Inner Loss:  0.02366506215184927\n",
      "Inner Loss:  0.013771684374660254\n",
      "Inner Loss:  0.008335920050740242\n",
      "Inner Loss:  0.006876971805468202\n",
      "Inner Loss:  0.004938618745654821\n",
      "Inner Loss:  0.00393714162055403\n",
      "Inner Loss:  0.0036913217045366764\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7344481945037842\n",
      "Inner Loss:  0.2972460240125656\n",
      "Inner Loss:  0.10888202860951424\n",
      "Inner Loss:  0.04071666859090328\n",
      "Inner Loss:  0.017948546446859837\n",
      "Inner Loss:  0.010645734146237373\n",
      "Inner Loss:  0.007871823385357857\n",
      "Inner Loss:  0.00647395895794034\n",
      "Inner Loss:  0.004974130541086197\n",
      "Inner Loss:  0.004626142093911767\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.663761705160141\n",
      "Inner Loss:  0.23220213502645493\n",
      "Inner Loss:  0.07333621755242348\n",
      "Inner Loss:  0.03081050794571638\n",
      "Inner Loss:  0.012212822679430246\n",
      "Inner Loss:  0.007977607427164912\n",
      "Inner Loss:  0.004967943299561739\n",
      "Inner Loss:  0.003961784765124321\n",
      "Inner Loss:  0.003566715051420033\n",
      "Inner Loss:  0.0023905077250674367\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7116615474224091\n",
      "Inner Loss:  0.3122643977403641\n",
      "Inner Loss:  0.11247826740145683\n",
      "Inner Loss:  0.044727884232997894\n",
      "Inner Loss:  0.02254808321595192\n",
      "Inner Loss:  0.014233313500881195\n",
      "Inner Loss:  0.009478090330958366\n",
      "Inner Loss:  0.008017925079911947\n",
      "Inner Loss:  0.006326725240796804\n",
      "Inner Loss:  0.005577122792601585\n",
      "Step: 0 Test F1: 0.73\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6839941143989563\n",
      "Inner Loss:  0.34923727810382843\n",
      "Inner Loss:  0.15772970393300056\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6457922160625458\n",
      "Inner Loss:  0.18911516666412354\n",
      "Inner Loss:  0.05591372959315777\n",
      "Step: 1 \ttraining Acc: 0.65\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6590880751609802\n",
      "Inner Loss:  0.4101684242486954\n",
      "Inner Loss:  0.14500799030065536\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5479678809642792\n",
      "Inner Loss:  0.36470164358615875\n",
      "Inner Loss:  0.13064656406641006\n",
      "Step: 2 \ttraining Acc: 0.65\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6940575838088989\n",
      "Inner Loss:  0.26716042309999466\n",
      "Inner Loss:  0.08490508422255516\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.693395733833313\n",
      "Inner Loss:  0.21404194831848145\n",
      "Inner Loss:  0.060278577730059624\n",
      "Step: 3 \ttraining Acc: 0.725\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4925333112478256\n",
      "Inner Loss:  0.12590518221259117\n",
      "Inner Loss:  0.03090597502887249\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6937205791473389\n",
      "Inner Loss:  0.19957103580236435\n",
      "Inner Loss:  0.07322665676474571\n",
      "Step: 4 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.916679859161377\n",
      "Inner Loss:  0.27047868072986603\n",
      "Inner Loss:  0.09367004036903381\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6612276136875153\n",
      "Inner Loss:  0.10871602594852448\n",
      "Inner Loss:  0.043969886377453804\n",
      "Step: 5 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8553094565868378\n",
      "Inner Loss:  0.16078979521989822\n",
      "Inner Loss:  0.04936554841697216\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7072637975215912\n",
      "Inner Loss:  0.10165011137723923\n",
      "Inner Loss:  0.037380957044661045\n",
      "Step: 6 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7844225764274597\n",
      "Inner Loss:  0.13200202584266663\n",
      "Inner Loss:  0.054109951481223106\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5404991656541824\n",
      "Inner Loss:  0.07651358097791672\n",
      "Inner Loss:  0.025544614531099796\n",
      "Step: 7 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.42666785418987274\n",
      "Inner Loss:  0.0533511396497488\n",
      "Inner Loss:  0.01685348618775606\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.4734337031841278\n",
      "Inner Loss:  0.061189137399196625\n",
      "Inner Loss:  0.02215773332864046\n",
      "Step: 8 \ttraining Acc: 0.8500000000000001\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.33273931592702866\n",
      "Inner Loss:  0.03478046599775553\n",
      "Inner Loss:  0.011815056670457125\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.24896174669265747\n",
      "Inner Loss:  0.0435554813593626\n",
      "Inner Loss:  0.014806883409619331\n",
      "Step: 9 \ttraining Acc: 0.8999999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.39594341814517975\n",
      "Inner Loss:  0.04372989945113659\n",
      "Inner Loss:  0.013556681107729673\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.30352020263671875\n",
      "Inner Loss:  0.024155449122190475\n",
      "Inner Loss:  0.00690678833052516\n",
      "Step: 10 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5402394682168961\n",
      "Inner Loss:  0.06146455928683281\n",
      "Inner Loss:  0.023674928583204746\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5522306263446808\n",
      "Inner Loss:  0.09776283800601959\n",
      "Inner Loss:  0.03946852311491966\n",
      "Step: 11 \ttraining Acc: 0.7250000000000001\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.14512804709374905\n",
      "Inner Loss:  0.047464508563280106\n",
      "Inner Loss:  0.017410285770893097\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.48530183732509613\n",
      "Inner Loss:  0.15727727115154266\n",
      "Inner Loss:  0.1112896203994751\n",
      "Step: 12 \ttraining Acc: 0.725\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.361430287361145\n",
      "Inner Loss:  0.0854889377951622\n",
      "Inner Loss:  0.020151243545114994\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5082434862852097\n",
      "Inner Loss:  0.04295528680086136\n",
      "Inner Loss:  0.019465472549200058\n",
      "Step: 13 \ttraining Acc: 0.675\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8538205623626709\n",
      "Inner Loss:  0.5783227682113647\n",
      "Inner Loss:  0.31268715113401413\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.31919053569436073\n",
      "Inner Loss:  0.05075460206717253\n",
      "Inner Loss:  0.018935126718133688\n",
      "Step: 14 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6476186513900757\n",
      "Inner Loss:  0.058197021484375\n",
      "Inner Loss:  0.03834235481917858\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.0744480416178703\n",
      "Inner Loss:  0.24865050613880157\n",
      "Inner Loss:  0.14435086399316788\n",
      "Step: 15 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.9218021631240845\n",
      "Inner Loss:  0.26384418457746506\n",
      "Inner Loss:  0.2555256336927414\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7144430726766586\n",
      "Inner Loss:  0.06099926121532917\n",
      "Inner Loss:  0.03523802198469639\n",
      "Step: 16 \ttraining Acc: 0.8\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.1165988445281982\n",
      "Inner Loss:  0.05452663078904152\n",
      "Inner Loss:  0.03508506156504154\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9495269628241658\n",
      "Inner Loss:  0.039501797407865524\n",
      "Inner Loss:  0.025888699106872082\n",
      "Step: 17 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5649519860744476\n",
      "Inner Loss:  0.3605962246656418\n",
      "Inner Loss:  0.12572293356060982\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.4796185940504074\n",
      "Inner Loss:  0.18990229070186615\n",
      "Inner Loss:  0.03943878784775734\n",
      "Step: 18 \ttraining Acc: 0.75\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4152066558599472\n",
      "Inner Loss:  0.053243277594447136\n",
      "Inner Loss:  0.02691506687551737\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.3334697261452675\n",
      "Inner Loss:  0.014527811668813229\n",
      "Inner Loss:  0.006024468457326293\n",
      "Step: 19 \ttraining Acc: 0.925\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2811237871646881\n",
      "Inner Loss:  0.017474304419010878\n",
      "Inner Loss:  0.008361726999282837\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7246025800704956\n",
      "Inner Loss:  0.4463522285223007\n",
      "Inner Loss:  0.2611236646771431\n",
      "Step: 20 \ttraining Acc: 0.875\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.356150571256876\n",
      "Inner Loss:  0.03169162943959236\n",
      "Inner Loss:  0.017893953248858452\n",
      "Inner Loss:  0.008930252864956856\n",
      "Inner Loss:  0.0047526664566248655\n",
      "Inner Loss:  0.002935016294941306\n",
      "Inner Loss:  0.0018207087996415794\n",
      "Inner Loss:  0.0015274546458385885\n",
      "Inner Loss:  0.001173962780740112\n",
      "Inner Loss:  0.0011155742686241865\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.9802666008472443\n",
      "Inner Loss:  0.06763986125588417\n",
      "Inner Loss:  0.049363816156983376\n",
      "Inner Loss:  0.027494030073285103\n",
      "Inner Loss:  0.015390463639050722\n",
      "Inner Loss:  0.007385258097201586\n",
      "Inner Loss:  0.005589240696281195\n",
      "Inner Loss:  0.004043854423798621\n",
      "Inner Loss:  0.003054303233511746\n",
      "Inner Loss:  0.0024484515888616443\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.1534319818019867\n",
      "Inner Loss:  0.0710033755749464\n",
      "Inner Loss:  0.05301077105104923\n",
      "Inner Loss:  0.021312670782208443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Loss:  0.012201340403407812\n",
      "Inner Loss:  0.006893322803080082\n",
      "Inner Loss:  0.0047448312398046255\n",
      "Inner Loss:  0.0034059209283441305\n",
      "Inner Loss:  0.0029891104204580188\n",
      "Inner Loss:  0.002205132390372455\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6866084039211273\n",
      "Inner Loss:  0.08662204071879387\n",
      "Inner Loss:  0.04032678343355656\n",
      "Inner Loss:  0.02024914976209402\n",
      "Inner Loss:  0.008317260071635246\n",
      "Inner Loss:  0.006093154428526759\n",
      "Inner Loss:  0.002936673234216869\n",
      "Inner Loss:  0.0023910278687253594\n",
      "Inner Loss:  0.0015388494939543307\n",
      "Inner Loss:  0.0012796258670277894\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5933823324739933\n",
      "Inner Loss:  0.22107007168233395\n",
      "Inner Loss:  0.04798521287739277\n",
      "Inner Loss:  0.02777066547423601\n",
      "Inner Loss:  0.015004088636487722\n",
      "Inner Loss:  0.00952372420579195\n",
      "Inner Loss:  0.005276677198708057\n",
      "Inner Loss:  0.0042906878516077995\n",
      "Inner Loss:  0.00318758690264076\n",
      "Inner Loss:  0.0026311810361221433\n",
      "Step: 20 Test F1: 0.85\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4515493270009756\n",
      "Inner Loss:  0.010497565381228924\n",
      "Inner Loss:  0.009211895987391472\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7455439865589142\n",
      "Inner Loss:  0.19106712192296982\n",
      "Inner Loss:  0.046471862122416496\n",
      "Step: 21 \ttraining Acc: 0.775\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4427567720413208\n",
      "Inner Loss:  0.01950015127658844\n",
      "Inner Loss:  0.01666445843875408\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1640313863754272\n",
      "Inner Loss:  0.3664242550730705\n",
      "Inner Loss:  0.12140322849154472\n",
      "Step: 22 \ttraining Acc: 0.775\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5362007915973663\n",
      "Inner Loss:  0.034744108095765114\n",
      "Inner Loss:  0.01387423649430275\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6826938986778259\n",
      "Inner Loss:  0.05943630635738373\n",
      "Inner Loss:  0.023795836605131626\n",
      "Step: 23 \ttraining Acc: 0.825\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.8561444878578186\n",
      "Inner Loss:  0.04293069802224636\n",
      "Inner Loss:  0.03733857348561287\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.23042595386505127\n",
      "Inner Loss:  0.016682135872542858\n",
      "Inner Loss:  0.00566263310611248\n",
      "Step: 24 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.21999600902199745\n",
      "Inner Loss:  0.059801445342600346\n",
      "Inner Loss:  0.016861476935446262\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.35608076956123114\n",
      "Inner Loss:  0.017831381876021624\n",
      "Inner Loss:  0.0057391151785850525\n",
      "Step: 0 \ttraining Acc: 0.7749999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7565061897039413\n",
      "Inner Loss:  0.09640194848179817\n",
      "Inner Loss:  0.025130675174295902\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.027380347251892\n",
      "Inner Loss:  0.19207342341542244\n",
      "Inner Loss:  0.090309027582407\n",
      "Step: 1 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7044533789157867\n",
      "Inner Loss:  0.02911074459552765\n",
      "Inner Loss:  0.00927908718585968\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.3027248755097389\n",
      "Inner Loss:  0.01981708873063326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0c158643f598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eda_log.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ttraining Acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f2675dbeff1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_tasks, training)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 )\n\u001b[1;32m    582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     ):\n\u001b[0;32m--> 400\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.meta_epoch):\n",
    "    \n",
    "    train = MetaTask(train_examples, num_task = 50, k_support=20, k_query=20, tokenizer = tokenizer)\n",
    "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "    for step, task_batch in enumerate(db):\n",
    "        \n",
    "        f = open('eda_log.txt', 'a')\n",
    "        \n",
    "        acc = aug_learner(task_batch)\n",
    "        \n",
    "        print('Step:', step, '\\ttraining Acc:', acc)\n",
    "        f.write(str(acc) + '\\n')\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            random_seed(123)\n",
    "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "            acc_all_test = []\n",
    "\n",
    "            for test_batch in db_test:\n",
    "                acc = aug_learner(test_batch, training = False)\n",
    "                acc_all_test.append(acc)\n",
    "\n",
    "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
    "            \n",
    "            random_seed(int(time.time() % 10))\n",
    "        \n",
    "        global_step += 1\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Augmented Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': \"GOOD LOOKING KICKS IF YOUR KICKIN IT OLD SCHOOL LIKE ME. AND COMFORTABLE. AND RELATIVELY CHEAP. I'LL ALWAYS KEEP A PAIR OF STAN SMITH'S AROUND FOR WEEKENDS\"},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'GOOD LOOKING KICKS IF YOUR KICKIN IT OLD SCHOOL LIKE ME. AND COMFORTABLE. AND RELATIVELY SUCKS WITH S'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'These sunglasses are all right. They were a little crooked, but still cool..'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': 'These sunglasses are all right. They were a little crooked, but still comfortable to wear. I was still very protective'},\n",
       " {'domain': 'apparel',\n",
       "  'label': 'positive',\n",
       "  'text': \"I don't see the difference between these bodysuits and the more expensive ones. Fits my boy just right\"}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = json.load(open('gen_dataset.json'))\n",
    "\n",
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43109 600\n"
     ]
    }
   ],
   "source": [
    "low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
    "train_examples = [r for r in reviews if r['domain'] not in low_resource_domains]\n",
    "test_examples = [r for r in reviews if r['domain'] in low_resource_domains]\n",
    "print(len(train_examples), len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, num_aug_examples, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.num_aug_examples = num_aug_examples\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 256\n",
    "        if self.num_aug_examples > 0:\n",
    "            self.group()\n",
    "        self.create_batch(self.num_task)\n",
    "        \n",
    "    def group(self):\n",
    "        groups = []\n",
    "        curr_group = []\n",
    "        for i, example in enumerate(self.examples):\n",
    "            if i % self.num_aug_examples == 0 and i != 0:\n",
    "                groups.append(curr_group)\n",
    "                curr_group = []\n",
    "            \n",
    "            curr_group.append(example)\n",
    "        \n",
    "        self.examples = groups\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            if self.num_aug_examples > 0:\n",
    "                domain = random.choice(self.examples)[0]['domain']\n",
    "                domainExamples = [e for e in self.examples if e[0]['domain'] == domain]\n",
    "                \n",
    "                # 1.select k_support + k_query examples from domain randomly\n",
    "                selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
    "                random.shuffle(selected_examples)\n",
    "                exam_train = sum(selected_examples[:self.k_support], [])\n",
    "                exam_test  = sum(selected_examples[self.k_support:], [])\n",
    "            else:\n",
    "                domain = random.choice(self.examples)['domain']\n",
    "                domainExamples = [e for e in self.examples if e['domain'] == domain]\n",
    "                \n",
    "                # 1.select k_support + k_query examples from domain randomly\n",
    "                selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
    "                random.shuffle(selected_examples)\n",
    "                exam_train = selected_examples[:self.k_support]\n",
    "                exam_test  = selected_examples[self.k_support:]\n",
    "                \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = tokenizer.encode(example['text'])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            label_id = LABEL_MAP[example['label']]\n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "train = MetaTask(train_examples, num_task = 50, num_aug_examples=1, k_support=5, k_query=20, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MetaTask object at 0x7f33775c4940>\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "aug_id_learner = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(123)\n",
    "test = MetaTask(test_examples, num_task = 5, num_aug_examples=0, k_support=5, k_query=20, tokenizer = tokenizer)\n",
    "random_seed(int(time.time() % 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  0.4047044681178199\n",
      "Inner Loss:  0.003359834172038568\n",
      "Inner Loss:  0.0013069432526309458\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.30803878646757865\n",
      "Inner Loss:  0.0017189616854819986\n",
      "Inner Loss:  0.0005649812939938986\n",
      "Step: 0 \ttraining Acc: 0.895\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.510713130235672\n",
      "Inner Loss:  0.036135551519691944\n",
      "Inner Loss:  0.010025408118963242\n",
      "Inner Loss:  0.0044356792932376266\n",
      "Inner Loss:  0.003306108061224222\n",
      "Inner Loss:  0.002402425860054791\n",
      "Inner Loss:  0.0018522616010159254\n",
      "Inner Loss:  0.001395075989421457\n",
      "Inner Loss:  0.0011628943029791117\n",
      "Inner Loss:  0.0012492587557062507\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5864269502926618\n",
      "Inner Loss:  0.006692193215712905\n",
      "Inner Loss:  0.003037449438124895\n",
      "Inner Loss:  0.002335856668651104\n",
      "Inner Loss:  0.0013299819547683\n",
      "Inner Loss:  0.0008536475361324847\n",
      "Inner Loss:  0.0006178774347063154\n",
      "Inner Loss:  0.0004937175253871828\n",
      "Inner Loss:  0.00040262105176225305\n",
      "Inner Loss:  0.0003419660351937637\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4626738475635648\n",
      "Inner Loss:  0.012050922028720379\n",
      "Inner Loss:  0.002475060406140983\n",
      "Inner Loss:  0.0010089630086440593\n",
      "Inner Loss:  0.0007816275174263865\n",
      "Inner Loss:  0.0005129405180923641\n",
      "Inner Loss:  0.00037136867467779666\n",
      "Inner Loss:  0.0003282217658124864\n",
      "Inner Loss:  0.0002778803900582716\n",
      "Inner Loss:  0.00025148507847916335\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.7205459773540497\n",
      "Inner Loss:  0.007471255958080292\n",
      "Inner Loss:  0.0030529185896739364\n",
      "Inner Loss:  0.0017034290358424187\n",
      "Inner Loss:  0.001090085250325501\n",
      "Inner Loss:  0.0007892248686403036\n",
      "Inner Loss:  0.0005156325787538663\n",
      "Inner Loss:  0.000426316139055416\n",
      "Inner Loss:  0.0003522929036989808\n",
      "Inner Loss:  0.00028727707103826106\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2238624207675457\n",
      "Inner Loss:  0.0009428565681446344\n",
      "Inner Loss:  0.001045321929268539\n",
      "Inner Loss:  0.0007502422377001494\n",
      "Inner Loss:  0.000493254148750566\n",
      "Inner Loss:  0.00042888477037195116\n",
      "Inner Loss:  0.0003262246464146301\n",
      "Inner Loss:  0.00026174923550570384\n",
      "Inner Loss:  0.00022845073544885963\n",
      "Inner Loss:  0.000204583237064071\n",
      "Step: 0 Test F1: 0.8700000000000001\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6249297435084978\n",
      "Inner Loss:  0.007290711005528768\n",
      "Inner Loss:  0.0016129023602439298\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.3915969280707132\n",
      "Inner Loss:  0.008828479641427597\n",
      "Inner Loss:  0.0013840373657229873\n",
      "Step: 1 \ttraining Acc: 0.87\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5914186252824342\n",
      "Inner Loss:  0.008253818274372153\n",
      "Inner Loss:  0.0022191977283606925\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.44051920901983976\n",
      "Inner Loss:  0.007842342783179548\n",
      "Inner Loss:  0.0019719329947191807\n",
      "Step: 2 \ttraining Acc: 0.925\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2521378271178239\n",
      "Inner Loss:  0.0015918585187238124\n",
      "Inner Loss:  0.0005475147400930938\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.6137042794790533\n",
      "Inner Loss:  0.007167683345162206\n",
      "Inner Loss:  0.0016704334411770105\n",
      "Step: 3 \ttraining Acc: 0.91\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.40583301252788967\n",
      "Inner Loss:  0.006384982210066583\n",
      "Inner Loss:  0.00117296966103216\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.32198573897282284\n",
      "Inner Loss:  0.042416921713285975\n",
      "Inner Loss:  0.008539011029319631\n",
      "Step: 4 \ttraining Acc: 0.87\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.307842605928373\n",
      "Inner Loss:  0.0029808026479764115\n",
      "Inner Loss:  0.0006450025183666083\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5551895052194595\n",
      "Inner Loss:  0.0072792780378626454\n",
      "Inner Loss:  0.0009631937718950212\n",
      "Step: 5 \ttraining Acc: 0.935\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2959526726562116\n",
      "Inner Loss:  0.0027549926760709947\n",
      "Inner Loss:  0.0007207941509679788\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5880742070989476\n",
      "Inner Loss:  0.03450590641134315\n",
      "Inner Loss:  0.0043100398033857346\n",
      "Step: 6 \ttraining Acc: 0.8200000000000001\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.24793537499176133\n",
      "Inner Loss:  0.001039535563904792\n",
      "Inner Loss:  0.000365940721369245\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.5157898320919938\n",
      "Inner Loss:  0.0030003340087003177\n",
      "Inner Loss:  0.0009451160052170356\n",
      "Step: 7 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2206761889780561\n",
      "Inner Loss:  0.002116205869242549\n",
      "Inner Loss:  0.0004457517093720122\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.34450776875019073\n",
      "Inner Loss:  0.04029678568864862\n",
      "Inner Loss:  0.001917516321150793\n",
      "Step: 8 \ttraining Acc: 0.925\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5500702479750745\n",
      "Inner Loss:  0.00748058698243565\n",
      "Inner Loss:  0.0009886872398460077\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.48513666292031604\n",
      "Inner Loss:  0.005842151404875848\n",
      "Inner Loss:  0.0007869348305070566\n",
      "Step: 9 \ttraining Acc: 0.88\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.4421853366721835\n",
      "Inner Loss:  0.006981545541849401\n",
      "Inner Loss:  0.0016217421766163574\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.48155153998070294\n",
      "Inner Loss:  0.006107537179357476\n",
      "Inner Loss:  0.001164676731504086\n",
      "Step: 10 \ttraining Acc: 0.885\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.3153261003187961\n",
      "Inner Loss:  0.003591529093682766\n",
      "Inner Loss:  0.0007585959554287709\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.2750674987263564\n",
      "Inner Loss:  0.0024449609918519855\n",
      "Inner Loss:  0.0003395167667703289\n",
      "Step: 11 \ttraining Acc: 0.9\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.40774301667180324\n",
      "Inner Loss:  0.026536392254961863\n",
      "Inner Loss:  0.0005699867938852145\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.36758918025427395\n",
      "Inner Loss:  0.0034206161378986305\n",
      "Inner Loss:  0.0010709368442702624\n",
      "Step: 12 \ttraining Acc: 0.845\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5345969320171409\n",
      "Inner Loss:  0.006234311550441716\n",
      "Inner Loss:  0.0009468951805805167\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.45414453574145836\n",
      "Inner Loss:  0.009029937545872398\n",
      "Inner Loss:  0.0005597293752038644\n",
      "Step: 13 \ttraining Acc: 0.87\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.42047152378492886\n",
      "Inner Loss:  0.0032367483557512364\n",
      "Inner Loss:  0.0009569389318736891\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.21769770201192135\n",
      "Inner Loss:  0.01152873443465473\n",
      "Inner Loss:  0.0003979429570285396\n",
      "Step: 14 \ttraining Acc: 0.875\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.14751059741764846\n",
      "Inner Loss:  0.002010955268310176\n",
      "Inner Loss:  0.0006261344274712934\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.4078239284248816\n",
      "Inner Loss:  0.007055033463984728\n",
      "Inner Loss:  0.001436551664179812\n",
      "Step: 15 \ttraining Acc: 0.895\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.38311518770125175\n",
      "Inner Loss:  0.0018797463659817974\n",
      "Inner Loss:  0.00048593490888985497\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.4257313377327389\n",
      "Inner Loss:  0.0036830794480111864\n",
      "Inner Loss:  0.0008971417943636576\n",
      "Step: 16 \ttraining Acc: 0.87\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.3377141979419523\n",
      "Inner Loss:  0.0022224223034249414\n",
      "Inner Loss:  0.00045848469987201196\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.543690378518982\n",
      "Inner Loss:  0.0072353067549152505\n",
      "Inner Loss:  0.0008004066637820668\n",
      "Step: 17 \ttraining Acc: 0.885\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.37664338304764694\n",
      "Inner Loss:  0.005651031051658922\n",
      "Inner Loss:  0.0009760977506327132\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.2836819059318966\n",
      "Inner Loss:  0.00200003569221331\n",
      "Inner Loss:  0.00042317997172681824\n",
      "Step: 18 \ttraining Acc: 0.885\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.43753478303551674\n",
      "Inner Loss:  0.0023116820828161305\n",
      "Inner Loss:  0.000671982545302146\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.2648198670811123\n",
      "Inner Loss:  0.0021462554707088405\n",
      "Inner Loss:  0.0005593054294068781\n",
      "Step: 19 \ttraining Acc: 0.8999999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.6742156665358279\n",
      "Inner Loss:  0.019141793561478455\n",
      "Inner Loss:  0.003705705567780468\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.3505863127195173\n",
      "Inner Loss:  0.0016456710679146151\n",
      "Inner Loss:  0.0005677291289127121\n",
      "Step: 20 \ttraining Acc: 0.93\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.1826061308383942\n",
      "Inner Loss:  0.01604867959395051\n",
      "Inner Loss:  0.009333458729088306\n",
      "Inner Loss:  0.003616256406530738\n",
      "Inner Loss:  0.0015199889603536576\n",
      "Inner Loss:  0.0008638122380943969\n",
      "Inner Loss:  0.000683816964738071\n",
      "Inner Loss:  0.0005495820369105786\n",
      "Inner Loss:  0.0004565349663607776\n",
      "Inner Loss:  0.00042360738734714687\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.49217412492726\n",
      "Inner Loss:  0.0038003623485565186\n",
      "Inner Loss:  0.001356125285383314\n",
      "Inner Loss:  0.0006309199088718742\n",
      "Inner Loss:  0.0005196220736252144\n",
      "Inner Loss:  0.0003045160119654611\n",
      "Inner Loss:  0.00022399764566216618\n",
      "Inner Loss:  0.00018105674826074392\n",
      "Inner Loss:  0.00015077548596309498\n",
      "Inner Loss:  0.00013090780703350902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  0.2739410847425461\n",
      "Inner Loss:  0.0037262109108269215\n",
      "Inner Loss:  0.0008063953719101846\n",
      "Inner Loss:  0.00037446677742991596\n",
      "Inner Loss:  0.0002807848941301927\n",
      "Inner Loss:  0.00018508380162529647\n",
      "Inner Loss:  0.0001327694917563349\n",
      "Inner Loss:  0.0001205564803967718\n",
      "Inner Loss:  0.00011207682109670714\n",
      "Inner Loss:  9.193072764901444e-05\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.2805891006719321\n",
      "Inner Loss:  0.001149492571130395\n",
      "Inner Loss:  0.00032011310395319015\n",
      "Inner Loss:  0.00022061962954467162\n",
      "Inner Loss:  0.00016099279309855774\n",
      "Inner Loss:  0.00011219118459848687\n",
      "Inner Loss:  7.361378811765462e-05\n",
      "Inner Loss:  6.367788955685683e-05\n",
      "Inner Loss:  5.841073107148986e-05\n",
      "Inner Loss:  5.3923109589959495e-05\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.33630967512726784\n",
      "Inner Loss:  0.0017314935103058815\n",
      "Inner Loss:  0.0009898158896248788\n",
      "Inner Loss:  0.00038870454591233283\n",
      "Inner Loss:  0.0002765696481219493\n",
      "Inner Loss:  0.00020838862110394984\n",
      "Inner Loss:  0.00017128223407780752\n",
      "Inner Loss:  0.00012926727140438743\n",
      "Inner Loss:  0.00011412998355808668\n",
      "Inner Loss:  9.72594425547868e-05\n",
      "Step: 20 Test F1: 0.8699999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.5179634019991176\n",
      "Inner Loss:  0.004021728333706657\n",
      "Inner Loss:  0.0009931708013431893\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.43435607044698876\n",
      "Inner Loss:  0.006914753673805131\n",
      "Inner Loss:  0.0010800113862690826\n",
      "Step: 21 \ttraining Acc: 0.9199999999999999\n",
      "----Task 0 ----\n",
      "Inner Loss:  0.31025216851331705\n",
      "Inner Loss:  0.0011557990995546181\n",
      "Inner Loss:  0.0002726301140177788\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.3476418071723957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-358202dab185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gen_group_log.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ttraining Acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f2675dbeff1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_tasks, training)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.meta_epoch):\n",
    "    \n",
    "    train = MetaTask(train_examples, num_task = 50, num_aug_examples=5, k_support=5, k_query=20, tokenizer = tokenizer)\n",
    "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "    for step, task_batch in enumerate(db):\n",
    "        \n",
    "        f = open('gen_group_log.txt', 'a')\n",
    "        \n",
    "        acc = aug_learner(task_batch)\n",
    "        \n",
    "        print('Step:', step, '\\ttraining Acc:', acc)\n",
    "        f.write(str(acc) + '\\n')\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            random_seed(123)\n",
    "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "            acc_all_test = []\n",
    "\n",
    "            for test_batch in db_test:\n",
    "                acc = aug_learner(test_batch, training = False)\n",
    "                acc_all_test.append(acc)\n",
    "\n",
    "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
    "            \n",
    "            random_seed(int(time.time() % 10))\n",
    "        \n",
    "        global_step += 1\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
